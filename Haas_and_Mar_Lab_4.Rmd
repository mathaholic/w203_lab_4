---
title: 'Lab 4: Healthy Momma, Healthy Baby'
author: "Krista Mar and Nikki Haas"
date: "12/1/2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### A Nice Introduction that Makes Us Sound Like Pros


According to the NIH, having a healthy pregancy is one of the best ways to promote a healthy birth and that getting early and regular prenatal care improves the chances of a healthy pregnancy.[1] According to Hack et all, while most low birth weight children will end up having normal outcomes, as a group they generally have more health issues than healthy weight babies[2].

Using data from the National Center for Health Statistics and from birth certificates, we will look at the impact of prenatal health care on health outcomes for newborn infants.

According to Montgomery, the Apgar scores are used as an evaluative measure to see if a newborn needs immediate attention. However, the using Apgar scores to attempt to predict long-term developmental outcomes of infants in not appropriates, so we will not be using Apgar scores in our outcome variable for newborn health. [3]

Therefore we will use birthweight as our outcome variable for our analysis based on historical research because of the limitations of our dataset.

Something about higher birthweight that talks about neural development [big babies, big brains](http://sciencenordic.com/birth-weight-predicts-brain-development)

### Step 1: Read in the Data

```{r}
load('/Users/nicholeh/student285/w203/w203_lab_4/bwght_w203.RData')
desc
```

### Step 2: Exploratory Data Analysis

First, get summary statistics on each element of the dataset:


```{r}
nrow(data)

summary(data)


```


##### *Response Variables*

The bwght, lbwght, omaps and fmaps variables are related to the health of the baby.

The first thing to check is if these variables are collinar.  We will omit bwghts as that is a function of lbwghts.

```{r}
library(ggplot2)
cor(data$omaps, data$fmaps, use = "complete.obs")
cor(data$lbwght, data$fmaps, use = "complete.obs")
p <- ggplot(data, aes(omaps, lbwght)) + geom_point(size = 0.25) + 
  geom_smooth(method = "lm", se = FALSE) + geom_point(aes(colour = fmaps)) +
  ggtitle("Scatterplot of log(weight) against One Minute APGAR test,\n
          with 5 minute APGAR test heatmap")
p
```


```{r}
p <- ggplot(data, aes(factor(fmaps), lbwght)) + geom_boxplot() 
p
```


Look at the extreme fmops case
```{r}
data[data$fmaps< 4,]
```


Looking at the data, we can be reasonably assured that the response variables are related, but not collinear. It may be best to make a combined variable of `fmaps` and `omaps` such as `mapscombined = fmaps + omaps`.  The difference would not make much sense compared to the sum; 10 - 10 and 2 - 2 are both zero, after all.  


##### *Regressors*

The variables monpre and npvis are related to the prenatal care given during pregnancy.  Let us review them for collinearity:
```{r}
cor(data$npvis, data$monpre, use = "complete.obs")
ggplot(data, aes(monpre, npvis)) + geom_point(size = 0.25) + 
  geom_smooth(method = "lm", se = FALSE) + geom_jitter()
ggplot(data, aes(factor(monpre), npvis)) + geom_boxplot() 
```

From this set, we can see that the data is not collinear, and indeed we can see that we might have some reporting errors.  5 mothers are listed as starting prenatal care in month 0 of their pregnancy, but they visited the doctor 0 times.  These probably denote missing information or an error in reporting.  Unfortunately, this data does show a definitive downward trend leading us to suspect that the number of visits is a function of month prenatal care began.  This makes sense intuitively; if a mother starts prenatal care in her 2nd month of pregnancy, she has ample time for frequent doctor visits.  However, if she starts her prenatal care towards the end of her pregnancy, she does not have enough time to visit the doctor as often as a woman who started in month 2.

```{r}
ggplot(data, aes(x=monpre)) + geom_histogram(aes(y = ..count..),bins = 10) +
  ggtitle("Month Prenatal Care began") 
ggplot(data, aes(x=sqrt(monpre))) + geom_histogram(aes(y = ..count..), bins = 10) +
  ggtitle("Month Prenatal Care Began, Half Power")
ggplot(data, aes(x=log(monpre))) + geom_histogram(aes(y = ..count..), bins = 10) + 
  ggtitle("Month Prenatal Care Began, Natural Log")
ggplot(data, aes(x=(monpre^2))) + geom_histogram(aes(y = ..count..), bins = 10) +
  ggtitle("Month Prenatal Care Began, Square Power")
ggplot(data, aes(x=npvis)) + geom_histogram(aes(y = ..count..), bins = 15) + 
  ggtitle("Number of Prenatal Visits")
```



All in all, the number of visits follows a mostly normal curve, and the square root of the month prenatal care began follow a mostly normal curve.  Then we say smart things about how that will all relate to each other.


### Step 3: Modeling 

##### Model 1: Basic Linear Model

```{r}
model1<-lm(bwght ~ monpre + npvis, data = data)
summary(model1)$r.squared
```
6 CLM assumptions: 

1) Linearity in parameters: We can assume this.

2) Random sampling of data: Not random because are not including still births or miscarriages.

3) No perfect co-linearity

```{r}
cor(data$monpre, data$npvis, use="complete.obs")
```
There is no perfect multicolineraity between our variables. With a correlation of -0.3061006, this shows that the number of prenatal visits is moderately negatively correlated to the month in whcih prenatal care started.

4) Zero conditional mean

```{r}
plot(model1, which=1)
```

Looking at the Residuals vs. Fitted plot shows that the zero conditional mean is met because the red line is approximately at 0.

5) Homoskedacity of errors

From the residuals vs. fitted plot, we can see that we do not have homoskedacity of erorrs because the data is not in an even band across the plot. This means that we'll have to white standard errors, which are roboust to heteroskadacity.

6) Errors are normally distributed

```{r}
par(mar = rep(2, 4))
plot(model1, which=2)
shapiro.test(model1$residuals)
```

Checking the normal Q-Q plot, it looks like our errors are roughly normally distributed.

Using the shapiro wilke test, we can reject the null hypothesis that the population has a normal distribution.

```{r}
library(lmtest)
library(sandwich)
coeftest(model1, vcov = vcovHC)
```

##### Model 2: An Alternate Main Model

The 1 minute and 5 minute APGAR scores on their own do not tell us much. As we can see from the heatmap on the first scatterplot, a baby who has a low one minute score tends to have a higher five minute score.  There are very few examples of a baby having a worse five minute score than a one minute score:

```{r}
nrow(data[!is.na(data$fmaps) < !is.na(data$omaps),])
```

However, we can get some information if we take the product of `omaps` and `fmaps` and then normalize it.  A baby that goes from 0 to 10 then would have an overal low score compared to a baby who started with a score of 10 and was still at 10 5 minutes later, so the difference doesn't make sense.


```{r}
data$product_apgarscores = data$omaps * data$fmaps
data$normalized_product_apgar =  (data$product_apgarscores- mean(!is.na(data$product_apgarscores)))/sd(!is.na(data$product_apgarscores))

a8 = lm(data$normalized_product_apgar~data$monpre + data$npvis)
a9 = lm(data$normalized_product_apgar~ data$npvis)

AIC(a8)
AIC(a9)
```


Model a8 has a nominally lower AIC score, so let's continue on with that one.

```{r}
summary(a8)
plot(a8)
```

We did not see very good results with the APGAR score variations, but as discussed in the introduction, we were expecting the baby's birth weight would have a better indication.

6 CLM assumptions: 

1) Linearity in parameters: We can assume this.

2) Random sampling of data: This data is not random because stillbirths are omitted.

3) No perfect co-linearity

As previously stated, our regressors do not have perfect collinearity.

4) Zero conditional mean


Looking at the Residuals vs. Fitted plot above shows that the zero conditional mean is met because the red line is approximately at 0 and has very little curvature.

5) Homoskedacity of errors

From the residuals vs. fitted plot, we can see that we do not have homoskedacity of erorrs because the data is not in an even band across the plot. This means that we'll have to use white standard errors, which are roboust to heteroskadacity.

6) Errors are normally distributed

```{r}
par(mar = rep(2, 4))
shapiro.test(a8$residuals)
```

From normal Q-Q plot, it looks like our errors are roughly normally distributed except at the very highest and very lowest percentiles.  This is to be expected in a dataset such as this.

Using the shapiro wilke test, we can reject the null hypothesis that the population has a normal distribution.

```{r}
library(lmtest)
library(sandwich)
coeftest(a8, vcov = vcovHC)
```

##### Model 3: Unbiased Covariants

```{r}
model3<-lm(bwght ~ monpre + npvis + cigs + drink + mage + male, data = data)
```
6 CLM assumptions: 

1) Linearity in parameters: We can assume this.

2) Random sampling of data: This data is not random because stillbirths are omitted.

3) No perfect co-linearity:  As previously stated, our regressors do not have perfect collinearity.

```{r}
cor(data[,c('monpre', 'npvis', 'cigs', 'drink', 'mage', 'male')], use="complete.obs")
```


4) Zero conditional mean

```{r}
plot(model3, which=1)
```
Looking at the Residuals vs. Fitted plot shows that the zero conditional mean is met because the red line is approximately at 0.

5) Homoskedacity of errors

From the residuals vs. fitted plot, we can see that we do not have homoskedacity of erorrs because the data is not in an even band across the plot. This means that we'll have to white standard errors, which are roboust to heteroskadacity.

6) Errors are normally distributed

```{r}
par(mar = rep(2, 4))
plot(model3, which=2)
shapiro.test(model3$residuals)
```
Checking the normal Q-Q plot, it looks like our errors are roughly normally distributed.

Using the shapiro wilke test, we can reject the null hypothesis that the population has a normal distribution.

```{r}
coeftest(model1, vcov=vcovHC)
coeftest(model3, vcov=vcovHC)
AIC(model1)
AIC(model3)
```


#### Model 4: Problematic Covariants

We will select the attributes of baby's gender and parent's race as well.  In the United States, it is a sad fact that minorities such as African Americans do not have adequate access to proper health care as often as non-minorities.  Their babies might not fare as well, and their mothers may not get the proper prenatal care.  

From all of the summaries, we can tell that the t-statistic for the `monpre` variable is not significant.  Thus, we cannot trust this particular regressor, and will omit it from this test.

```{r}
c1 = lm(data$bwght ~ data$npvis + data$male +
          data$mblck + data$fblck)
summary(c1)
AIC(c1)
plot(c1)
```


6 CLM assumptions: 

1) Linearity in parameters: We can assume this.

2) Random sampling of data: This data is not random because stillbirths are omitted.

3) No perfect co-linearity in regressors:

```{r}
cor(data[,c('npvis', 'mblck', 'fblck', 'male')], use="complete.obs")
```
As previously stated, our regressors do not have perfect collinearity.

4) Zero conditional mean

Looking at the Residuals vs. Fitted plot above shows that the zero conditional mean has not been met because the red line shows curvature for larger babies.

5) Homoskedacity of errors

From the residuals vs. fitted plot, we can see that we do not have homoskedacity of erorrs because the data is not in an even band across the plot. This means that we'll have to use white standard errors, which are roboust to heteroskadacity.

6) Errors are normally distributed

```{r}
par(mar = rep(2, 4))
shapiro.test(c1$residuals)
```

From normal Q-Q plot, it looks like our errors are roughly normally distributed except at the very lowest percentiles.  This is to be expected in a dataset such as this.

Using the shapiro wilke test, we can reject the null hypothesis that the population has a normal distribution.

```{r}
library(lmtest)
library(sandwich)
coeftest(c1, vcov = vcovHC)
```

As we were hoping with such biased data, we can see that the race of the parents is not statistically significant so it is inappropriate to include it in our model.

### Step 4: CLM and the Models


### Step 5: Regression Tables and Model Analysis

```{r, results='asis'}
library(stargazer)

se.model1 = sqrt(diag(vcovHC(model1)))
se.a8 = sqrt(diag(vcovHC(a8)))
se.model3 = sqrt(diag(vcovHC(model3)))
se.c1 = sqrt(diag(vcovHC(c1)))

stargazer(model1,a8,model3,c1, type = "latex", omit.stat = "f", 
          se = list(se.model1, se.a8, se.model3, se.c1),
          star.cutoffs = c(0.05, 0.01, 0.001),
          table.placement = '!h')
```

See table 1 on the next page.

```{r}
AIC(model1)
AIC(model3)
AIC(c1)
```

From the Akaike Information Criterion test, we see that `model3` is the best option for a linear model predicting the health of the baby.  Model3 has the highest adjusted R^2, showing that virutally 2% of all variability in the baby's health indicators can be determined by the months prenatal visits started, number of prenatal visits, the mother's smoking and driking habits, the mother's age, and the baby's gender.  As always, `monpre` was not a statistically significant regressor, and neither was the mother's age or drinking habits.  In words, we can say if the baby is a boy we can expect he will weigh 80.937 grams more than if he is a girl, for every year older his mother is, he will weight 5.317 grams more, for every alcoholic drink his mother inbibes per week he will weigh 14.050 grams less, for every cigarette his mother smokes per day, he will weigh 11.229 grams less, for each prenatal visit, he will weight 15.505 more, and for each month the mother waits to to start her prenatal care, the baby weight 20.901 grams more.  Just writing it out like that gives us good grounds to completely ignore the `monpre` variable.  

\pagebreak

### Step 6: Causality

We choose to operationalize infant health by birthweight. There are many other factors that influence birthweight that are not captured in this data set, which leads to omitted variable bias.

1) Mother's weight is a strong predictor for newborn weight.

2) Socioeconomic status of mother.

3) Having more than one baby at a time reduces the weight of each baby. (E.g. twins will be smaller)

4)

### Biases and Limitation

This data is extremely biased in that no still births were included in our dataset.  It is a sad fact in the United States that over 2 in 1,000 births are stillbirths[5].  Since we do not know the prenatal care data for stillbirths, we cannot completely guage how much prenatal care contributes to a child's health at birth.


In addition, it appears that there is little correlation between the Apgar score and the later health of the baby.  The Apar is only meant to be used in the context of emergency situations.  In this manner, looking at a baby's weight will give us deeper insight into the baby's overall health.

No miscarriages were included in the data, so this further biases our data.

Using birthweight as a proxy for infant health was the best that we could do given our data set, but is by no means a comprehensive view on an infants' health.

### Step 7: Conclusion

Prenatal care, as shown by number of prenatal care visits has a positive impact on birthweight. Other explanatory factors are mother's cig consumption, which has a negative impact on birthweigth. Being male has a positive impact on birthweight.

References

[1]https://www.nichd.nih.gov/health/topics/pregnancy/conditioninfo/pages/prenatal-care.aspx

[2]https://www.ncbi.nlm.nih.gov/pubmed/7543353

[3]https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1595023/
[4]http://ije.oxfordjournals.org/content/30/6/1233.long

[5]https://www.washingtonpost.com/news/wonk/wp/2014/09/29/our-infant-mortality-rate-is-a-national-embarrassment/?utm_term=.58dedfd178fd
